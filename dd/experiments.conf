best {
  data_dir = /data/sxl180006/codi2021/conllua_dir  # Edit this

  # Computation limits.
  max_top_antecedents = 50
  max_training_sentences = 1
  top_span_ratio = 0.4
  max_num_extracted_spans = 3900
  max_num_speakers = 20
  max_segment_len = 256

  # Learning
  bert_learning_rate = 1e-5
  task_learning_rate = 2e-4
  loss_type = marginalized  # {marginalized, hinge}
  mention_loss_coef = 0
  false_new_delta = 1.5  # For loss_type = hinge
  adam_eps = 1e-6
  adam_weight_decay = 1e-2
  warmup_ratio = 0.1
  max_grad_norm = 1  # Set 0 to disable clipping
  gradient_accumulation_steps = 1

  # Model hyperparameters.
  coref_depth = 1  # when 1: no higher order (except for cluster_merging)
  higher_order = attended_antecedent # {attended_antecedent, max_antecedent, entity_equalization, span_clustering, cluster_merging}
  coarse_to_fine = true
  fine_grained = true
  dropout_rate = 0.3
  ffnn_size = 1000
  ffnn_depth = 1
  cluster_ffnn_size = 1000   # For cluster_merging
  cluster_reduce = mean  # For cluster_merging
  easy_cluster_first = false  # For cluster_merging
  cluster_dloss = false  # cluster_merging
  num_epochs = 30
  feature_emb_size = 20
  max_span_width = 30
  use_metadata = true
  use_features = true
  use_segment_distance = true
  model_heads = true
  use_width_prior = true  # For mention score
  use_distance_prior = true  # For mention-ranking score
  use_5_subtokens_for_type = false
  use_4_dep_features_for_type = false
  do_type_prediction = false
  num_entity_types = 2

  # config
  max_sentence_distance_between_anaphor_and_antecedent = 0
  use_sentence_distance_as_feature = false
  use_coarse_for_modified_fast_score = false
  use_modified_fast_score = false
  use_modified_slow_score = false
  use_4_new_features = false
  use_dummy_antecedent = true
  use_new_num_features = false
  use_rule_based_prediction = false
  use_dep_parsing = false
  constraint_antecedent_utter = no_type_constraint
  use_utterace_span_from_file = Neither
  use_extra_features_based_on_anaphor = false
  use_extra_features_based_on_antecedent = false
  
  inference_filter_num_of_words_equals_zero = false
  inference_filter_non_gold_anaphors = false
  inference_filter_dependency_parsing = false
  inference_filter_antecedent_raw = 0
  inference_filter_antecedent_new = 0
  use_content_word_overlap = true

  # Other.
  conll_eval_path = ${best.data_dir}/dev.coref-hoi.CONLL  # gold_conll file for dev
  conll_test_path = ${best.data_dir}/test.coref-hoi.CONLL  # gold_conll file for test
  genres = ["bc", "bn", "mz", "nw", "pt", "tc", "wb"]
  entity_type_list = entity_type_list.txt
  pos_tags_list = pos_tags.txt
  dep_tags_list = dep_tags.txt
  eval_frequency = 1000
  report_frequency = 100
  log_root = ${best.data_dir}
  non_overlaps = false
  save_top_k = 2
  prediction_only = false
  predict_singletons = false
  training_phase = true
  cache_training_set = false
  evaluate_on_test_set_too = false
  gold_anaphor_setting = false
}

bert_base = ${best}{
  num_docs = 2802
  bert_learning_rate = 1e-05
  task_learning_rate = 2e-4
  max_segment_len = 128
  ffnn_size = 3000
  cluster_ffnn_size = 3000
  max_training_sentences = 1
  bert_tokenizer_name = bert-base-cased
  bert_pretrained_name_or_path = bert-base-cased
}

train_bert_base = ${bert_base}{
}

train_bert_base_ml0_d1 = ${train_bert_base}{
  mention_loss_coef = 0
  coref_depth = 1
}

train_bert_base_ml0_d2 = ${train_bert_base}{
  mention_loss_coef = 0
  coref_depth = 2
}

bert_large = ${best}{
  num_docs = 2802
  bert_learning_rate = 1e-05
  task_learning_rate = 2e-4
  max_segment_len = 384
  ffnn_size = 3000
  cluster_ffnn_size = 3000
  max_training_sentences = 1
  bert_tokenizer_name = bert-base-cased
  bert_pretrained_name_or_path = bert-large-cased
}

train_bert_large = ${bert_large}{
}

train_bert_large_ml0_d1 = ${train_bert_large}{
  mention_loss_coef = 0
  coref_depth = 1
}

train_bert_large_ml0_d2 = ${train_bert_large}{
  mention_loss_coef = 0
  coref_depth = 2
}

spanbert_base = ${best}{
  num_docs = 2802
  bert_learning_rate = 2e-05
  task_learning_rate = 0.0001
  max_segment_len = 384
  ffnn_size = 3000
  cluster_ffnn_size = 3000
  max_training_sentences = 1
  bert_tokenizer_name = bert-base-cased
  bert_pretrained_name_or_path = SpanBERT/spanbert-base-cased
}

train_spanbert_base = ${spanbert_base}{
}

debug_spanbert_base = ${train_spanbert_base}{
}

train_spanbert_base_ml0_d1 = ${train_spanbert_base}{
  mention_loss_coef = 0
  coref_depth = 1
}

train_spanbert_base_ml0_lr2e-4_d1 = ${train_spanbert_base}{
  mention_loss_coef = 0
  task_learning_rate = 2e-4
  coref_depth = 1
}

spanbert_large = ${best}{
  num_docs = 2802
  bert_learning_rate = 1e-05
  task_learning_rate = 0.0003
  max_segment_len = 512
  ffnn_size = 3000
  cluster_ffnn_size = 3000
  max_training_sentences = 3
  bert_tokenizer_name = bert-base-cased
  bert_pretrained_name_or_path = SpanBERT/spanbert-large-cased
}

train_spanbert_large = ${spanbert_large}{
}

train_spanbert_large_ml0_d1 = ${train_spanbert_large}{
  mention_loss_coef = 0
  coref_depth = 1
}

train_spanbert_large_ml0_lr_d1 = ${train_spanbert_large}{
  mention_loss_coef = 0
  bert_learning_rate = 2e-05
  task_learning_rate = 2e-4
  coref_depth = 1
}

train_spanbert_large_ml0_d2 = ${train_spanbert_large}{
  mention_loss_coef = 0
  coref_depth = 2
}

train_spanbert_large_ml0_lr_d2 = ${train_spanbert_large}{
  mention_loss_coef = 0
  bert_learning_rate = 2e-05
  task_learning_rate = 2e-4
  coref_depth = 2
}

train_spanbert_large_ml0_sc = ${train_spanbert_large}{
  mention_loss_coef = 0
  coref_depth = 2
  higher_order = span_clustering
}

train_spanbert_large_ml0_cm_fn1000 = ${train_spanbert_large}{
  mention_loss_coef = 0
  coref_depth = 1
  higher_order = cluster_merging
  cluster_ffnn_size = 1000
}

train_spanbert_large_ml0_cm_fn1000_dloss = ${train_spanbert_large}{
  mention_loss_coef = 0
  coref_depth = 1
  higher_order = cluster_merging
  cluster_ffnn_size = 1000
  cluster_dloss = true
}

train_spanbert_large_ml0_cm_fn1000_e1st = ${train_spanbert_large}{
  mention_loss_coef = 0
  coref_depth = 1
  higher_order = cluster_merging
  cluster_ffnn_size = 1000
  easy_cluster_first = true
}

train_spanbert_large_ml0_cm_fn1000_e1st_dloss = ${train_spanbert_large}{
  mention_loss_coef = 0
  coref_depth = 1
  higher_order = cluster_merging
  cluster_ffnn_size = 1000
  easy_cluster_first = true
  cluster_dloss = true
}

train_spanbert_large_ml0_cm_fn1000_max = ${train_spanbert_large}{
  mention_loss_coef = 0
  coref_depth = 1
  higher_order = cluster_merging
  cluster_ffnn_size = 1000
  cluster_reduce = max
}

train_spanbert_large_ml0_cm_fn1000_max_dloss = ${train_spanbert_large}{
  mention_loss_coef = 0
  coref_depth = 1
  higher_order = cluster_merging
  cluster_ffnn_size = 1000
  cluster_reduce = max
  cluster_dloss = true
}

train_spanbert_large_ml0_cm_fn1000_max_e1st = ${train_spanbert_large}{
  mention_loss_coef = 0
  coref_depth = 1
  higher_order = cluster_merging
  cluster_ffnn_size = 1000
  cluster_reduce = max
  easy_cluster_first = true
}

train_spanbert_large_ml0_cm_fn1000_max_e1st_dloss = ${train_spanbert_large}{
  mention_loss_coef = 0
  coref_depth = 1
  higher_order = cluster_merging
  cluster_ffnn_size = 1000
  cluster_reduce = max
  easy_cluster_first = true
  cluster_dloss = true
}

train_spanbert_large_ml1_d1 = ${train_spanbert_large}{
  mention_loss_coef = 1
  coref_depth = 1
}

FullModel_pred_NoNull_AllDatasets_0dot5 = ${train_spanbert_large}{
  max_training_sentences = 12
  eval_frequency = 500
  max_top_antecedents = 10

  num_epochs = 30

  mention_loss_coef = 1
  coref_depth = 1
  type_loss_coef = 0.5
  top_span_ratio = 1
  max_span_width = 300

  train_dir = all_train_dev_2022_content.jsonlines
  dev_dir = none
  test_dir = none

  max_sentence_distance_between_anaphor_and_antecedent = 10
  use_coarse_for_modified_fast_score = false
  use_modified_fast_score = false
  use_modified_slow_score = false
  use_5_subtokens_for_type = false
  use_4_new_features = false
  use_dummy_antecedent = false
  use_new_num_features = false
  use_rule_based_prediction = false
  use_dep_parsing = false
  use_extra_features_based_on_anaphor = true
  use_extra_features_based_on_antecedent = true
  use_sentence_distance_as_feature = true
  do_type_prediction = true
  num_entity_types = 2
  

  save_top_k = 99
  debug = false
  prediction_only = true
  use_utterace_span_from_file = Both
  constraint_antecedent_utter = constraint_antecedent_utter
  entity_type_list = entity_type_list.txt
  gold_conll = NONE
  evaluate_on_test_set_too = true
}

FullModel_pred_NoNull_AllDatasets_800 = ${train_spanbert_large}{
  max_training_sentences = 12
  eval_frequency = 500
  max_top_antecedents = 10

  num_epochs = 30

  mention_loss_coef = 1
  coref_depth = 1
  type_loss_coef = 800
  top_span_ratio = 1
  max_span_width = 300

  train_dir = all_train_dev_2022.jsonlines
#   train_dir = arddnaacl_pred_itself_dep10_alldatasets.jsonlines
  dev_dir = none
  test_dir = none

  max_sentence_distance_between_anaphor_and_antecedent = 10
  use_coarse_for_modified_fast_score = false
  use_modified_fast_score = false
  use_modified_slow_score = false
  use_5_subtokens_for_type = false
  use_4_new_features = false
  use_dummy_antecedent = false
  use_new_num_features = false
  use_rule_based_prediction = false
  use_dep_parsing = false
  use_extra_features_based_on_anaphor = true
  use_extra_features_based_on_antecedent = true
  use_sentence_distance_as_feature = true
  do_type_prediction = true
  num_entity_types = 2
  

  save_top_k = 99
  debug = false
  prediction_only = true
  use_utterace_span_from_file = Both
  constraint_antecedent_utter = constraint_antecedent_utter
  entity_type_list = entity_type_list.txt
  gold_conll = NONE
  evaluate_on_test_set_too = true
}

FullModel_pred_NoNull_AllDatasets_goldm_800 = ${FullModel_pred_NoNull_AllDatasets_800}{
  train_dir = all_train_dev_2022_content.goldset.jsonlines
}

FullModel_pred_NoNull_AllDatasets_goldm_100 = ${FullModel_pred_NoNull_AllDatasets_goldm_800}{
  type_loss_coef = 100
}
FullModel_pred_NoNull_AllDatasets_goldm_0dot5 = ${FullModel_pred_NoNull_AllDatasets_goldm_800}{
  type_loss_coef = 0.5
}
FullModel_pred_NoNull_AllDatasets_goldm_500 = ${FullModel_pred_NoNull_AllDatasets_goldm_800}{
  type_loss_coef = 500
}

FullModel_golda_NoNull_AllDatasets = ${FullModel_pred_NoNull_AllDatasets_goldm_800}{
  gold_anaphor_setting = true
  do_type_prediction = false
  constraint_antecedent_utter = constraint_antecedent_utter
  num_epochs = 21
  train_dir = all_train_dev_2022_content.goldana.removedExcessiveUtt.content.jsonlines
}
FullModel_golda_NoNull_5_AllDatasets = ${FullModel_golda_NoNull_AllDatasets}{
  num_epochs = 25
  max_sentence_distance_between_anaphor_and_antecedent = 5
}

FullModel_pred_NoNull_AllDatasets_extralight_0dot5 = ${train_spanbert_large}{
  max_training_sentences = 12
  eval_frequency = 500
  max_top_antecedents = 10

  num_epochs = 30

  mention_loss_coef = 1
  coref_depth = 1
  type_loss_coef = 0.5
  top_span_ratio = 1
  max_span_width = 300

  train_dir = all_train_dev_2022_extralight_content.jsonlines
  dev_dir = none
  test_dir = none

  max_sentence_distance_between_anaphor_and_antecedent = 10
  use_coarse_for_modified_fast_score = false
  use_modified_fast_score = false
  use_modified_slow_score = false
  use_5_subtokens_for_type = false
  use_4_new_features = false
  use_dummy_antecedent = false
  use_new_num_features = false
  use_rule_based_prediction = false
  use_dep_parsing = false
  use_extra_features_based_on_anaphor = true
  use_extra_features_based_on_antecedent = true
  use_sentence_distance_as_feature = true
  do_type_prediction = true
  num_entity_types = 2
  

  save_top_k = 99
  debug = false
  prediction_only = true
  use_utterace_span_from_file = Both
  constraint_antecedent_utter = constraint_antecedent_utter
  entity_type_list = entity_type_list.txt
  gold_conll = NONE
  evaluate_on_test_set_too = true
}